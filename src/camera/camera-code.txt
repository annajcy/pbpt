

## camera.hpp

```hpp
/**
 * @file
 * @brief Core camera interface and common sampling structures.
 */
#pragma once

#include "geometry/ray.hpp"

namespace pbpt::camera {

/**
 * @brief Sample on the film plane and (optionally) the lens.
 *
 * This structure encodes where a ray should intersect the virtual film
 * (p_film) in raster coordinates and, for thin-lens cameras, where on
 * the lens aperture the ray should originate (p_lens).
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
struct CameraSample{
    /// Film sample position in raster or viewport space.
    math::Point<T, 2> p_film{};
    /// Lens sample position for thin-lens cameras (ignored for pinhole).
    ///
    /// Typically this is a point in a 2D unit disk (later scaled by the
    /// lens radius) used to simulate finite aperture and depth of field.
    math::Point<T, 2> p_lens{};

    /**
     * @brief Create a camera sample for a pinhole camera.
     *
     * The lens point is fixed at the origin and is ignored by pinhole
     * camera models.
     *
     * @param p_film Film sample position.
     * @return CameraSample with `p_lens` set to (0, 0).
     */
    static CameraSample<T> create_pinhole_sample(const math::Point<T, 2>& p_film) {
        return CameraSample<T>{p_film, math::Point<T, 2>(0, 0)};
    }

    /**
     * @brief Create a camera sample for a thin-lens camera.
     *
     * Both film and lens sample positions are specified, allowing thin-lens
     * camera models to compute rays that correctly simulate depth of field.
     *
     * @param p_film Film sample position.
     * @param p_lens Lens sample position.
     * @return CameraSample with both film and lens positions set.
     */
    static CameraSample<T> create_thinlens_sample(const math::Point<T, 2>& p_film, const math::Point<T, 2>& p_lens) {
        return CameraSample<T>{p_film, p_lens};
    }
};

/**
 * @brief CRTP base class for camera models.
 *
 * Concrete camera implementations derive from this template and provide
 * three implementation methods:
 * - `generate_ray_impl` to generate a primary ray.
 * - `generate_differential_ray_impl` to generate ray differentials for
 *   texture filtering and derivatives.
 * - `film_resolution_impl` to report the film resolution in pixels.
 *
 * The base class forwards calls to these methods using CRTP.
 *
 * @tparam Derived Concrete camera type.
 * @tparam T       Scalar type (e.g. float or double).
 */
template<typename Derived, typename T>
class Camera {
public:
    /**
     * @brief Generate a primary camera ray from a sample.
     *
     * @param sample Film and lens sample positions.
     * @return Primary ray in camera space.
     */
    geometry::Ray<T, 3> generate_ray(const CameraSample<T>& sample) const {
        return as_derived().generate_ray_impl(sample);
    }

    /**
     * @brief Generate a ray with associated differentials.
     *
     * Ray differentials are used to approximate how rays change with
     * small perturbations in the film plane, which is important for
     * texture filtering and anti-aliasing.
     *
     * @param sample Film and lens sample positions.
     * @return Ray with differentials in camera space.
     */
    geometry::RayDifferential<T, 3> generate_differential_ray(const CameraSample<T>& sample) const {
        return as_derived().generate_differential_ray_impl(sample);
    }

    /**
     * @brief Get the film resolution in pixels.
     *
     * @return 2D vector (width, height) in pixels.
     */
    math::Vector<int, 2> film_resolution() const {
        return as_derived().film_resolution_impl();
    }

private:
    /// Access the derived implementation (non-const).
    constexpr Derived& as_derived() noexcept {
        return static_cast<Derived&>(*this);
    }

    /// Access the derived implementation (const).
    constexpr const Derived& as_derived() const noexcept {
        return static_cast<const Derived&>(*this);
    }
};

} // namespace pbpt::camera

```

## camera_system.hpp

```hpp
#pragma once

#include "camera/render_transform.hpp"
namespace pbpt::camera {

template<typename T, typename CameraType, typename FilmType, typename PixelFilterType>
class CameraSystem {
private:
    CameraType m_camera;
    FilmType m_film;
    PixelFilterType m_pixel_filter;
    RenderTransform<T> m_render_transform;

public:
    CameraSystem(
        const CameraType& camera,
        const FilmType& film,
        const PixelFilterType& pixel_filter
    ) : m_camera(camera), m_film(film), m_pixel_filter(pixel_filter) {}

    const CameraType& camera() const {
        return m_camera;
    }

    const FilmType& film() const {
        return m_film;
    }

    const PixelFilterType& pixel_filter() const {
        return m_pixel_filter;
    }
};

};
```

## film.hpp

```hpp
/**
 * @file
 * @brief Film abstractions for accumulating radiance samples into pixels.
 */
#pragma once

#include <array>
#include <cstddef>
#include <vector>

#include "math/point.hpp"
#include "math/utils.hpp"
#include "math/vector.hpp"

#include "radiometry/color.hpp"
#include "radiometry/sampled_spectrum.hpp"

namespace pbpt::camera {

/**
 * @brief CRTP base class for film implementations.
 *
 * A film represents a 2D discrete sensor that accumulates radiance samples
 * at integer pixel coordinates. Derived classes implement how spectral
 * radiance is converted to stored pixel values (e.g. RGB, G-buffer, etc.).
 *
 * @tparam Derived Concrete film type.
 * @tparam T       Scalar type (e.g. float or double).
 */
template<typename Derived, typename T>
class Film {
private:
    /// Pixel resolution (width, height).
    math::Vector<int, 2> m_resolution{};
    /// Physical size of the film (e.g. in millimeters) in x and y.
    math::Vector<T, 2> m_physical_size{};

public:
    /**
     * @brief Construct a film with given resolution and physical size.
     *
     * @param resolution    Pixel resolution (width, height).
     * @param physical_size Physical size of the film in x and y.
     */
    Film(const math::Vector<int, 2>& resolution, const math::Vector<T, 2>& physical_size)
        : m_resolution(resolution), m_physical_size(physical_size) {}

    /// Get the film resolution in pixels.
    const math::Vector<int, 2>& resolution() const { return m_resolution; }
    /// Get the physical size of the film.
    const math::Vector<T, 2>& physical_size() const { return m_physical_size; }

    /**
     * @brief Add a spectral sample using Monte Carlo sampled wavelengths.
     *
     * The derived film decides how to convert the sampled spectral radiance
     * plus sampling PDF into pixel values (e.g. via a pixel sensor model).
     *
     * @tparam N Number of spectral samples.
     * @param p_film      Pixel coordinate.
     * @param radiance    Sampled spectral radiance.
     * @param wavelengths Sampled wavelengths.
     * @param pdf         PDF for each wavelength sample.
     * @param weight      Monte Carlo weight applied to this sample.
     */
    template<int N>
    void add_sample(
        const math::Point<int, 2>& p_film,
        const radiometry::SampledSpectrum<T, N>& radiance,
        const radiometry::SampledWavelength<T, N>& wavelengths,
        const radiometry::SampledPdf<T, N>& pdf,
        T weight
    ) {
        as_derived().template add_sample_impl<N>(
            p_film, 
            radiance, 
            wavelengths, 
            pdf, 
            weight
        );
    }

    /**
     * @brief Add a spectral sample using a full spectrum type.
     *
     * This overload is used when the spectrum type can be integrated
     * directly without Monte Carlo sampling of wavelengths.
     *
     * @tparam SpectrumType Type representing a spectral radiance distribution.
     * @param p_film   Pixel coordinate.
     * @param radiance Spectral radiance.
     * @param weight   Monte Carlo weight applied to this sample.
     */
    template<typename SpectrumType>
    void add_sample(
        const math::Point<int, 2>& p_film,
        const SpectrumType& radiance,
        T weight
    ) {
        as_derived().add_sample_impl(
            p_film, 
            radiance, 
            weight
        );
    }

    /// Access the derived implementation (non-const).
    Derived& as_derived() {
        return static_cast<Derived&>(*this);
    }

    /// Access the derived implementation (const).
    const Derived& as_derived() const {
        return static_cast<const Derived&>(*this);
    }
};

/**
 * @brief Film that accumulates RGB values produced by a pixel sensor.
 *
 * This film stores for each pixel a running weighted sum of RGB values
 * and the sum of weights. The final pixel color is obtained by dividing
 * the accumulated RGB sum by the weight sum.
 *
 * @tparam T              Scalar type (e.g. float or double).
 * @tparam PixelSensorType Pixel sensor type used to convert spectra to RGB.
 */
template<typename T, typename PixelSensorType>
class HDRFilm : public Film<HDRFilm<T, PixelSensorType>, T> {
public:

    /**
     * @brief Per-pixel accumulation buffer.
     *
     * Stores a running sum of RGB values (in double precision for better
     * numerical stability) and the total weight of samples contributing
     * to this pixel.
     */
    struct Pixel {
        /// Accumulated weighted RGB sum.
        std::array<T, 3> rgb_sum{0.0, 0.0, 0.0};
        /// Sum of sample weights.
        double weight_sum{0.0};

        /**
         * @brief Accumulate a single RGB sample into this pixel.
         *
         * @param rgb    Sampled RGB value.
         * @param weight Monte Carlo weight associated with the sample.
         */
        void add_sample(const radiometry::RGB<T>& rgb, T weight) {
            double w = static_cast<double>(weight);
            rgb_sum[0] += w * static_cast<double>(rgb[0]);
            rgb_sum[1] += w * static_cast<double>(rgb[1]);
            rgb_sum[2] += w * static_cast<double>(rgb[2]);
            weight_sum += w;
        }

        /**
         * @brief Resolve the final pixel color.
         *
         * Returns the average RGB value by dividing the accumulated sum
         * by the total weight. If no samples have contributed, returns
         * black (0, 0, 0).
         *
         * @return Final RGB value for this pixel.
         */
        radiometry::RGB<T> resolve() const {
            if (math::is_zero(weight_sum)) {
                return radiometry::RGB<T>(T(0), T(0), T(0));
            }
            double inv_weight = 1.0 / weight_sum;
            return radiometry::RGB<T>(
                static_cast<T>(rgb_sum[0] * inv_weight),
                static_cast<T>(rgb_sum[1] * inv_weight),
                static_cast<T>(rgb_sum[2] * inv_weight)
            );
        }

        /// Reset accumulation to zero.
        void clear() {
            rgb_sum = {0.0, 0.0, 0.0};
            weight_sum = 0.0;
        }   
    };

private:
    /// Pixel sensor used to convert spectral radiance to RGB.
    PixelSensorType m_pixel_sensor;
    /// Linear buffer of per-pixel accumulators.
    std::vector<Pixel> m_pixels;

public:
    /**
     * @brief Construct an RGB film with a given resolution and sensor.
     *
     * @param resolution    Pixel resolution (width, height).
     * @param physical_size Physical size of the film.
     * @param pixel_sensor  Pixel sensor used for spectral to RGB conversion.
     */
    HDRFilm(
        const math::Vector<int, 2>& resolution,
        const math::Vector<T, 2>& physical_size,
        const PixelSensorType& pixel_sensor
    ) : Film<HDRFilm<T, PixelSensorType>, T>(resolution, physical_size),
        m_pixel_sensor(pixel_sensor) {
        int width = this->resolution().x();
        int height = this->resolution().y();
        math::assert_if(width <= 0 || height <= 0, "RGBFilm requires a positive resolution");
        m_pixels.resize(static_cast<std::size_t>(width) * static_cast<std::size_t>(height));
    }

    /**
     * @brief Implementation of spectral sample accumulation for sampled spectra.
     *
     * The spectrum is first converted to sensor RGB using the pixel sensor
     * and then mapped into the target color space. The resulting RGB is
     * accumulated into the pixel buffer.
     */
    template<int N>
    void add_sample_impl(
        const math::Point<int, 2>& p_film,
        const radiometry::SampledSpectrum<T, N>& radiance,
        const radiometry::SampledWavelength<T, N>& wavelengths,
        const radiometry::SampledPdf<T, N>& pdf,
        T weight
    ) {
        Pixel& pixel = pixel_at(p_film);
        auto sensor_rgb = m_pixel_sensor.template radiance_to_sensor_rgb<N>(radiance, wavelengths, pdf);
        auto display_rgb = m_pixel_sensor.sensor_rgb_to_color_space_rgb(sensor_rgb);
        pixel.add_sample(display_rgb, weight);
    }

    /**
     * @brief Implementation of spectral sample accumulation for full spectra.
     *
     * This overload uses the pixel sensor's direct spectrum integration
     * method to convert radiance to sensor RGB, then to display RGB.
     */
    template<typename SpectrumType>
    void add_sample_impl(
        const math::Point<int, 2>& p_film,
        const SpectrumType& radiance,
        T weight
    ) {
        Pixel& pixel = pixel_at(p_film);
        auto sensor_rgb = m_pixel_sensor.radiance_to_sensor_rgb(radiance);
        auto display_rgb = m_pixel_sensor.sensor_rgb_to_color_space_rgb(sensor_rgb);
        pixel.add_sample(display_rgb, weight);
    }

    /**
     * @brief Directly accumulate an RGB sample into the film.
     *
     * This is useful when the radiance has already been converted to
     * display RGB outside the film (for example, when debugging).
     */
    void add_color_sample(
        const math::Point<int, 2>& p_film,
        const radiometry::RGB<T>& rgb,
        T weight
    ) {
        Pixel& pixel = pixel_at(p_film);
        pixel.add_sample(rgb, weight);
    }

    /**
     * @brief Get the resolved RGB value for a pixel.
     *
     * @param p_film Pixel coordinate.
     * @return Averaged RGB value for that pixel.
     */
    radiometry::RGB<T> get_pixel_rgb(const math::Point<int, 2>& p_film) const {
        return pixel_at(p_film).resolve();
    }

    /// Clear all pixel accumulators.
    void clear() {
        for (auto& pixel : m_pixels) {
            pixel.clear();
        }
    }

    /// Get a const reference to the pixel sensor.
    const PixelSensorType& pixel_sensor() const {
        return m_pixel_sensor;
    }

    /// Get a mutable reference to the pixel sensor.
    PixelSensorType& pixel_sensor() {
        return m_pixel_sensor;
    }

    /// Get a const reference to the internal pixel buffer.
    const std::vector<Pixel>& pixels() const {
        return m_pixels;
    }

    /// Get a mutable reference to the internal pixel buffer.
    std::vector<Pixel>& pixels() {
        return m_pixels;
    }

private:
    /// Access a pixel by integer coordinate (with bounds checks).
    Pixel& pixel_at(const math::Point<int, 2>& p_film) {
        math::assert_if(!is_pixel_in_bounds(p_film), "Film sample coordinate out of range");
        return m_pixels[pixel_index(p_film)];
    }

    /// Access a pixel by integer coordinate (const, with bounds checks).
    const Pixel& pixel_at(const math::Point<int, 2>& p_film) const {
        math::assert_if(!is_pixel_in_bounds(p_film), "Film sample coordinate out of range");
        return m_pixels[pixel_index(p_film)];
    }

    /// Check if a pixel coordinate lies inside the film bounds.
    bool is_pixel_in_bounds(const math::Point<int, 2>& p_film) const {
        int width = this->resolution().x();
        int height = this->resolution().y();
        return p_film.x() >= 0 && p_film.x() < width &&
               p_film.y() >= 0 && p_film.y() < height;
    }

    /// Convert a 2D pixel coordinate to a linear index.
    std::size_t pixel_index(const math::Point<int, 2>& p_film) const {
        std::size_t width = static_cast<std::size_t>(this->resolution().x());
        return static_cast<std::size_t>(p_film.y()) * width + static_cast<std::size_t>(p_film.x());
    }
};

//TODO G-Buffer film

};

```

## pixel_filter.hpp

```hpp
/**
 * @file
 * @brief Pixel reconstruction filters and related sampling utilities.
 */
#pragma once

#include <array>
#include <cmath>
#include <vector>
#include "math/point.hpp"
#include "sampler/2d.hpp"
#include "math/vector.hpp"

namespace pbpt::camera {

/**
 * @brief CRTP base class for pixel filters.
 *
 * Provides a common interface for sampling film positions within a pixel
 * according to a reconstruction filter, and for evaluating the corresponding
 * filter kernel and sampling PDFs. The concrete sampling behavior is
 * implemented in the derived class.
 *
 * @tparam T       Scalar type (e.g. float or double).
 * @tparam Derived Concrete filter type using CRTP.
 */
template <typename T, typename Derived>
class PixelFilter {
public:
    /**
     * @brief Filtered camera sample on the film plane.
     *
     * Represents a sampled position on the film, the corresponding offset
     * from the pixel center, and the Monte Carlo weight associated with
     * the reconstruction filter.
     */
    struct FilteredCameraSample {
        /// Sampled film position (pixel center plus offset).
        math::Point<T, 2> film_position;
        /// Offset from the pixel center in film coordinates.
        math::Vector<T, 2> offset;
        /// Monte Carlo weight, typically f(x) / q(x) where f(x) is the reconstruction filter and q(x) is the sampling PDF.
        T weight;
    };

protected:
    /**
     * @brief Filter radius in pixels.
     *
     * For box filters this is the half width; for tent filters this is
     * the support radius. The filter kernel is usually non-zero only
     * inside [-m_filter_radius, m_filter_radius]^2.
     */
    T m_filter_radius{};
    
public:
    /**
     * @brief Construct a pixel filter with a given radius.
     * @param filter_radius Filter radius in pixels (default 0.5).
     */
    PixelFilter(T filter_radius = T(0.5)) : m_filter_radius(filter_radius) {}

    /**
     * @brief Get the filter radius (read-only).
     * @return Constant reference to the filter radius.
     */
    const T& filter_radius() const {
        return m_filter_radius;
    }

    /**
     * @brief Get the filter radius (writable).
     * @return Reference to the filter radius.
     */
    T& filter_radius() {
        return m_filter_radius;
    }

    /**
     * @brief Evaluate the sampling PDF of an offset.
     *
     * For a given offset from the pixel center, returns the probability
     * density under the sampling strategy used by the filter.
     *
     * @param offset Offset from the pixel center.
     * @return Probability density at the given offset.
     */
    T sample_offset_pdf(const math::Vector<T, 2>& offset) {
        return as_derived().sample_offset_pdf_impl(offset);
    }

    /**
     * @brief Evaluate the filter kernel at a given offset.
     *
     * @param offset Offset from the pixel center.
     * @return Filter kernel value f(offset).
     */
    T filter_kernel(const math::Vector<T, 2>& offset) {
        return as_derived().filter_kernel_impl(offset);
    }

    /**
     * @brief Sample an offset from the filter's sampling distribution.
     *
     * @param uv 2D uniform sample in [0, 1)^2.
     * @return Offset relative to the pixel center.
     */
    math::Vector<T, 2> sample_offset(const math::Point<T, 2>& uv) {
        return as_derived().sample_offset_impl(uv);
    }

    /**
     * @brief Sample a filtered film position within a pixel.
     *
     * Uses @p uv to sample an offset according to the filter's sampling
     * distribution, then computes the Monte Carlo weight f/q where f is
     * the filter kernel and q is the sampling PDF.
     *
     * @param pixel Integer pixel coordinates.
     * @param uv    2D uniform sample in [0, 1)^2.
     * @return Filtered camera sample containing film position, offset and weight.
     */
    FilteredCameraSample sample_film_position(
        const math::Point<int, 2>& pixel,
        const math::Point<T, 2>& uv
    ) const {
        math::Point<T, 2> pixel_center(
            static_cast<T>(pixel.x()) + T(0.5),
            static_cast<T>(pixel.y()) + T(0.5)
        );
        // Sample offset according to q, then compute MC weight f/q.
        auto offset = as_derived().sample_offset_impl(uv);
        T f = as_derived().filter_kernel_impl(offset);
        T q = as_derived().sample_offset_pdf_impl(offset);
        T weight = (q > T(0)) ? (f / q) : T(0);
        return {pixel_center + offset, offset, weight};
    }

    /**
     * @brief Generate a regular grid of UV samples over [0, 1)^2.
     *
     * The samples correspond to the centers of a @p u_sample_count by
     * @p v_sample_count grid in UV space.
     *
     * @param u_sample_count Number of samples in the u direction.
     * @param v_sample_count Number of samples in the v direction.
     * @return Vector of UV points in [0, 1)^2.
     */
    std::vector<math::Point<T, 2>> get_uv_samples(
        int u_sample_count, 
        int v_sample_count
    ) const {
        std::vector<math::Point<T, 2>> uvs;
        double u_stride = 1.0 / static_cast<T>(u_sample_count);
        double v_stride = 1.0 / static_cast<T>(v_sample_count);
        uvs.reserve(u_sample_count * v_sample_count);
        for (int v = 0; v < v_sample_count; ++v) {
            for (int u = 0; u < u_sample_count; ++u) {
                T u_coord = 0.5 * u_stride + static_cast<T>(u) * u_stride;
                T v_coord = 0.5 * v_stride + static_cast<T>(v) * v_stride;
                uvs.emplace_back(u_coord, v_coord);
            }
        }
        return uvs;
    }

    /**
    * @brief Generate a regular grid of UV samples over [0, 1)^2.
    *
    * The samples correspond to the centers of a @p USampleCount by
    * @p VSampleCount grid in UV space.
    *
    * @tparam USampleCount Number of samples in the u direction.
    * @tparam VSampleCount Number of samples in the v direction.
    * @return Array of UV points in [0, 1)^2.
    */
    template<int USampleCount, int VSampleCount>
    std::array<math::Point<T, 2>, USampleCount * VSampleCount> get_uv_samples() const {
        std::array<math::Point<T, 2>, USampleCount * VSampleCount> uvs{};
        double u_stride = 1.0 / static_cast<T>(USampleCount);
        double v_stride = 1.0 / static_cast<T>(VSampleCount);
        for (int v = 0; v < VSampleCount; ++v) {
            for (int u = 0; u < USampleCount; ++u) {
                T u_coord = 0.5 * u_stride + static_cast<T>(u) * u_stride;
                T v_coord = 0.5 * v_stride + static_cast<T>(v) * v_stride;
                uvs[v * USampleCount + u] = math::Point<T, 2>(u_coord, v_coord);
            }
        }
        return uvs;
    }

    /**
     * @brief Generate multiple filtered camera samples within a pixel.
     *
     * Builds a regular UV grid using @c get_uv_samples() and, for each
     * UV point, calls @c sample_film_position() to obtain a filtered
     * camera sample.
     *
     * @param pixel           Integer pixel coordinates.
     * @param u_sample_count  Number of samples in the u direction.
     * @param v_sample_count  Number of samples in the v direction.
     * @return Vector of filtered camera samples for the pixel.
     */
    std::vector<FilteredCameraSample> get_camera_samples(
        const math::Point<int, 2>& pixel,
        int u_sample_count,
        int v_sample_count
    ) const {
        std::vector<FilteredCameraSample> samples;
        auto uvs = get_uv_samples(u_sample_count, v_sample_count);
        samples.reserve(uvs.size());
        for (const auto& uv : uvs) {
            samples.push_back(sample_film_position(pixel, uv));
        }
        return samples;
    }

    /**
     * @brief Generate multiple filtered camera samples within a pixel.
     * @tparam USampleCount  Number of samples in the u direction.
     * @tparam VSampleCount  Number of samples in the v direction.
     * @param pixel           Integer pixel coordinates.
     * @return Array of filtered camera samples for the pixel.
     */
    template<int USampleCount, int VSampleCount>
    std::array<FilteredCameraSample, USampleCount * VSampleCount> get_camera_samples(
        const math::Point<int, 2>& pixel
    ) const {
        std::array<FilteredCameraSample, USampleCount * VSampleCount> samples;
        auto uvs = get_uv_samples<USampleCount, VSampleCount>();
        for (int i = 0; i < USampleCount * VSampleCount; ++i) {
            samples[i] = sample_film_position(pixel, uvs[i]);
        }
        return samples;
    }

protected:
    /**
     * @brief Access the derived implementation (non-const).
     *
     * Helper for CRTP to dispatch calls to the derived class.
     *
     * @return Reference to the derived type.
     */
    Derived& as_derived() {
        return static_cast<Derived&>(*this);
    }

    /**
     * @brief Access the derived implementation (const).
     *
     * Helper for CRTP to dispatch calls to the derived class.
     *
     * @return Const reference to the derived type.
     */
    const Derived& as_derived() const {
        return static_cast<const Derived&>(*this);
    }
};

/**
 * @brief Box (uniform) pixel filter.
 *
 * Uses a uniform sampling distribution over a square region
 * [-r, r] × [-r, r], and a filter kernel that is 1 inside this region
 * and 0 outside.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template <typename T>
class BoxFilter : public PixelFilter<T, BoxFilter<T>> {
    friend class PixelFilter<T, BoxFilter<T>>;
    using PixelFilter<T, BoxFilter<T>>::m_filter_radius;

public:
    /**
     * @brief Construct a box filter with a given radius.
     *
     * The radius controls the half-width of the filter support in
     * both x and y directions.
     *
     * @param filter_radius Half-width of the box filter in pixels.
     */
    BoxFilter(T filter_radius = T(0.5)) : PixelFilter<T, BoxFilter<T>>(filter_radius) {}

    /**
     * @brief Sample an offset uniformly within the box support.
     *
     * @param uv 2D uniform sample in [0, 1)^2.
     * @return Offset in [-r, r] × [-r, r].
     */
    math::Vector<T, 2> sample_offset_impl(const math::Point<T, 2>& uv) const {
        return sampler::sample_uniform_2d(
            uv, 
            math::Vector<T, 2>(-m_filter_radius, m_filter_radius), 
            math::Vector<T, 2>(-m_filter_radius, m_filter_radius)
        ).to_vector();
    }

    /**
     * @brief PDF of the uniform box sampling distribution.
     *
     * @param offset Offset from the pixel center.
     * @return Probability density for @p offset.
     */
    T sample_offset_pdf_impl(const math::Vector<T, 2>& offset) const {
        return sampler::sample_uniform_2d_pdf(
            math::Point<T, 2>(offset.x(), offset.y()),
            math::Vector<T, 2>(-m_filter_radius, m_filter_radius),
            math::Vector<T, 2>(-m_filter_radius, m_filter_radius)
        );
    }

    /**
     * @brief Box filter kernel.
     *
     * Returns 1 inside the support [-r, r] × [-r, r] and 0 outside.
     *
     * @param offset Offset from the pixel center.
     * @return Filter kernel value.
     */
    T filter_kernel_impl(const math::Vector<T, 2>& offset) const {
        if (std::abs(offset.x()) > m_filter_radius || std::abs(offset.y()) > m_filter_radius) {
            return T(0);
        }
        return T(1);
    }
};

/**
 * @brief Tent (linear) pixel filter.
 *
 * Uses a separable 2D tent kernel
 * f(x, y) = (1 - |x|/r) (1 - |y|/r) for |x|, |y| ≤ r and 0 otherwise.
 * Sampling and PDF are matched to this kernel for importance sampling.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template <typename T>
class TentFilter : public PixelFilter<T, TentFilter<T>> {
    friend class PixelFilter<T, TentFilter<T>>;
    using PixelFilter<T, TentFilter<T>>::m_filter_radius;

public:
    /**
     * @brief Construct a tent filter with a given radius.
     *
     * The radius controls the support of the tent kernel in x and y
     * directions.
     *
     * @param filter_radius Support radius of the tent filter in pixels.
     */
    TentFilter(T filter_radius = T(0.5)) : PixelFilter<T, TentFilter<T>>(filter_radius) {}

    /**
     * @brief Sample an offset according to a 2D tent distribution.
     *
     * @param uv 2D uniform sample in [0, 1)^2.
     * @return Offset sampled from the tent distribution.
     */
    math::Vector<T, 2> sample_offset_impl(const math::Point<T, 2>& uv) const {
        return sampler::sample_tent_2d(
            uv, 
            m_filter_radius, 
            m_filter_radius
        ).to_vector();
    }

    /**
     * @brief PDF of the 2D tent sampling distribution.
     *
     * @param offset Offset from the pixel center.
     * @return Probability density for @p offset.
     */
    T sample_offset_pdf_impl(const math::Vector<T, 2>& offset) const {
        return sampler::sample_tent_2d_pdf(
            math::Point<T, 2>(offset.x(), offset.y()), 
            m_filter_radius, 
            m_filter_radius
        );
    }

    /**
     * @brief Tent filter kernel.
     *
     * Returns (1 - |x|/r) (1 - |y|/r) when |x|, |y| ≤ r and 0 otherwise.
     *
     * @param offset Offset from the pixel center.
     * @return Filter kernel value.
     */
    T filter_kernel_impl(const math::Vector<T, 2>& offset) const {
        T ax = std::abs(offset.x()) / m_filter_radius;
        T ay = std::abs(offset.y()) / m_filter_radius;
        if (ax > T(1) || ay > T(1)) {
            return T(0);
        }
        return (T(1) - ax) * (T(1) - ay);
    }
};


//TODO: Add GaussianFilter

}

```

## pixel_sensor.hpp

```hpp
/**
 * @file
 * @brief Pixel sensor model mapping spectral radiance to RGB/XYZ/color-space values.
 *
 * The sensor is modeled as a 3-channel device with spectral sensitivities
 * and an associated scene illuminant. Spectral radiance at a pixel is
 * integrated against these curves to produce sensor RGB values, which are
 * then linearly transformed into CIE XYZ and finally into a target RGB
 * color space.
 */
#pragma once

#include "math/matrix.hpp"
#include "math/point.hpp"
#include "radiometry/color.hpp"
#include "radiometry/color_space.hpp"
#include "radiometry/constant/swatch_reflectances_spectrum.hpp"
#include "radiometry/constant/xyz_spectrum.hpp"
#include "radiometry/sampled_spectrum.hpp"

namespace pbpt::camera {

/**
 * @brief Pixel sensor that converts spectral radiance into RGB/XYZ values.
 *
 * The sensor is parameterized by a scene illuminant, a reference standard
 * illuminant, a 3-channel spectral response, and a target RGB color space.
 * Conceptually, for each sensor channel we integrate the product of
 * spectral radiance, scene illuminant and that channel's sensitivity
 * over wavelength to obtain the channel response.
 *
 * The resulting sensor RGB triplet is mapped to CIE XYZ via a 3×3 matrix
 * that is typically estimated by fitting a linear transform between
 * simulated sensor responses for a set of reference reflectances and their
 * ground-truth XYZ under a standard illuminant.
 *
 * @tparam T                           Scalar type (e.g. float or double).
 * @tparam SceneIlluminantSpectrumType Spectrum type of the scene illuminant.
 * @tparam StandardIlluminantSpectrumType Spectrum type of the standard illuminant.
 * @tparam SensorResponseSpectrumType  Spectrum type describing the 3-channel sensor response.
 */
template<typename T, typename SceneIlluminantSpectrumType, typename StandardIlluminantSpectrumType, typename SensorResponseSpectrumType>
class PixelSensor {
private:
    /// Spectral power distribution of the scene illuminant used for rendering.
    SceneIlluminantSpectrumType m_scene_illuminant;
    /// Reference standard illuminant used to define the XYZ color space (e.g. D65).
    StandardIlluminantSpectrumType m_standard_illuminant;   
    /// Target RGB color space into which XYZ values are converted.
    radiometry::RGBColorSpace<T> m_color_space;
    /// Spectral response of the 3 sensor channels.
    radiometry::ResponseSpectrum<SensorResponseSpectrumType> m_sensor_response;
    /// 3×3 matrix that maps sensor RGB to CIE XYZ tristimulus values.
    math::Matrix<T, 3, 3> m_sensor_rgb_to_xyz{};
    /// Global scale factor applied to sensor RGB (e.g. exposure/image ratio).
    T m_image_ratio{1.0};

public:
    /**
     * @brief Construct a pixel sensor with an explicit sensor response and calibration.
     *
     * This constructor simulates sensor RGB responses and reference XYZ values
     * for a set of standard reflectance swatches, then solves for a 3×3 matrix
     * that best maps the simulated sensor RGBs to the reference XYZ values
     * in a least-squares sense.
     *
     * For each swatch, we compute:
     * - a 3-component sensor RGB response by integrating reflectance *
     *   scene_illuminant * sensor_response over wavelength;
     * - a 3-component reference XYZ response by integrating reflectance *
     *   standard_illuminant * XYZ color-matching functions.
     *
     * The calibration matrix is then chosen so that M * rgb_camera
     * is as close as possible (in least-squares sense) to xyz_output
     * over all swatches.
     *
     * @param scene_illuminant    Spectrum of the scene illuminant.
     * @param standard_illuminant Spectrum of the reference standard illuminant.
     * @param color_space         Target RGB color space definition.
     * @param sensor_response     Spectral response of the 3-channel sensor.
     * @param image_ratio         Global scale factor applied to sensor RGB.
     */
    PixelSensor(
        const SceneIlluminantSpectrumType& scene_illuminant,
        const StandardIlluminantSpectrumType& standard_illuminant,
        const radiometry::RGBColorSpace<T>& color_space,
        const radiometry::ResponseSpectrum<SensorResponseSpectrumType>& sensor_response,
        T image_ratio = T{1.0}
    ) : m_scene_illuminant(scene_illuminant),
        m_standard_illuminant(standard_illuminant),
        m_sensor_response(sensor_response), 
        m_color_space(color_space),
        m_image_ratio(image_ratio) {

            math::Matrix<T, 3, radiometry::constant::swatch_reflectances_count> rgb_camera{};
            for (int i = 0; i < radiometry::constant::swatch_reflectances_count; ++i) {
                auto reflectance = radiometry::constant::swatch_reflectances<T>[i];
                auto rgb = radiometry::project_reflectance<T, radiometry::RGB>(
                    reflectance, 
                    m_scene_illuminant, 
                    m_sensor_response, 
                    false
                );
                rgb_camera[0][i] = rgb[0];
                rgb_camera[1][i] = rgb[1];
                rgb_camera[2][i] = rgb[2];
            }

            math::Matrix<T, 3, radiometry::constant::swatch_reflectances_count> xyz_output{};
            for (int i = 0; i < radiometry::constant::swatch_reflectances_count; ++i) {
                auto reflectance = radiometry::constant::swatch_reflectances<T>[i];
                auto xyz = radiometry::project_reflectance<T, radiometry::XYZ>(
                    reflectance, 
                    m_standard_illuminant, 
                    radiometry::ResponseSpectrum<radiometry::constant::XYZSpectrumType<T>>(
                        radiometry::constant::CIE_X<T>,
                        radiometry::constant::CIE_Y<T>,
                        radiometry::constant::CIE_Z<T>
                    ), 
                    false);
                xyz_output[0][i] = xyz[0];
                xyz_output[1][i] = xyz[1];
                xyz_output[2][i] = xyz[2];  
            }

            m_sensor_rgb_to_xyz = math::solve_LMS(
                rgb_camera,
                xyz_output
            );
    }

    /**
     * @brief Construct a sensor that behaves like an XYZ colorimeter with white balance.
     *
     * In this overload the sensor response is set to the CIE XYZ color-matching
     * functions, and the 3×3 matrix is a white-balance transform that adapts
     * from the scene illuminant chromaticity to the standard illuminant
     * chromaticity. Conceptually, this is similar to von Kries chromatic
     * adaptation: we find a 3×3 transform that maps the white
     * chromaticity of the scene illuminant to the white chromaticity of
     * the standard illuminant by scaling and rotating the tristimulus
     * axes in a linear color space.
     *
     * @param scene_illuminant    Spectrum of the scene illuminant.
     * @param standard_illuminant Spectrum of the reference standard illuminant.
     * @param color_space         Target RGB color space definition.
     * @param image_ratio         Global scale factor applied to sensor RGB.
     */
    PixelSensor(
        const SceneIlluminantSpectrumType& scene_illuminant,
        const StandardIlluminantSpectrumType& standard_illuminant,
        const radiometry::RGBColorSpace<T>& color_space,
        T image_ratio = T{1.0}
    ) : m_scene_illuminant(scene_illuminant),
        m_standard_illuminant(standard_illuminant),
        m_sensor_response(
            radiometry::ResponseSpectrum<radiometry::constant::XYZSpectrumType<T>>(
                radiometry::constant::CIE_X<T>,
                radiometry::constant::CIE_Y<T>,
                radiometry::constant::CIE_Z<T>
            )
        ), 
        m_color_space(color_space),
        m_image_ratio(image_ratio) {
            math::Point<T, 2> src_chroma_xy = radiometry::XYZ<T>::from_illuminant(m_scene_illuminant).to_xy();
            math::Point<T, 2> dst_chroma_xy = radiometry::XYZ<T>::from_illuminant(m_standard_illuminant).to_xy();
            auto wb_matrix = radiometry::white_balance<T>(src_chroma_xy, dst_chroma_xy);
            m_sensor_rgb_to_xyz = wb_matrix;
    }

    /**
     * @brief Construct a sensor directly aligned with CIE XYZ under a standard illuminant.
     *
     * Here the scene illuminant is assumed to be the same as the standard
     * illuminant and the sensor response is again the CIE XYZ color-matching
     * functions. The sensor RGB triplet is already an XYZ tristimulus, so
     * the calibration matrix is simply the 3×3 identity matrix.
     *
     * @param standard_illuminant Spectrum of the standard illuminant.
     * @param color_space         Target RGB color space definition.
     * @param image_ratio         Global scale factor applied to sensor RGB.
     */
    PixelSensor(
        const StandardIlluminantSpectrumType& standard_illuminant,
        const radiometry::RGBColorSpace<T>& color_space,
        T image_ratio = T{1.0}
    ) : m_scene_illuminant(standard_illuminant),
        m_standard_illuminant(standard_illuminant),
        m_sensor_response(
            radiometry::ResponseSpectrum<radiometry::constant::XYZSpectrumType<T>>(
                radiometry::constant::CIE_X<T>,
                radiometry::constant::CIE_Y<T>,
                radiometry::constant::CIE_Z<T>
            )
        ), 
        m_color_space(color_space),
        m_image_ratio(image_ratio) {
            m_sensor_rgb_to_xyz = math::Matrix<T, 3, 3>::identity();
    }

    /**
     * @brief Convert spectral radiance to sensor RGB using full spectra.
     *
     * Conceptually this evaluates, for each channel, the integral over
     * wavelength of:
     * radiance(lambda) * scene_illuminant(lambda) * sensor_response(lambda).
     * The resulting sensor RGB is then multiplied by the image_ratio factor.
     *
     * @tparam SpectrumType Type representing a spectral radiance distribution.
     * @param radiance      Spectral radiance at the pixel.
     * @return Sensor RGB triplet proportional to the captured signal.
     */
    template<typename SpectrumType>
    radiometry::RGB<T> radiance_to_sensor_rgb(
        const SpectrumType& radiance
    ) const {
        auto sensor_rgb = radiometry::project_emission<T, radiometry::RGB, SpectrumType, SceneIlluminantSpectrumType, SensorResponseSpectrumType>(
            radiance,
            m_scene_illuminant,
            m_sensor_response
        );
        return sensor_rgb * m_image_ratio;
    }

    /**
     * @brief Convert sampled spectral radiance to sensor RGB using Monte Carlo integration.
     *
     * This overload approximates the spectral integral by Monte Carlo sampling.
     * For N sampled wavelengths lambda_i with sampling PDF p(lambda_i),
     * the sensor RGB is estimated by averaging
     *   radiance(lambda_i) * scene_illuminant(lambda_i)
     *   * sensor_response(lambda_i) / p(lambda_i)
     * over all samples. The result is then scaled by the image_ratio factor.
     *
     * @tparam N Number of spectral samples.
     * @param radiance     Sampled spectral radiance values.
     * @param wavelengths  Sampled wavelengths corresponding to @p radiance.
     * @param pdf          Sampling PDF for each wavelength sample.
     * @return Sensor RGB triplet estimated from the sampled spectrum.
     */
    template<int N>
    radiometry::RGB<T> radiance_to_sensor_rgb(
        const radiometry::SampledSpectrum<T, N>& radiance, 
        const radiometry::SampledWavelength<T, N>& wavelengths,
        const radiometry::SampledPdf<T, N> &pdf
    ) const {
        auto sampled_illuminant = m_scene_illuminant.template sample<N>(wavelengths);
        auto sensor_rgb = radiometry::project_sampled_emission<T, radiometry::RGB, N, radiometry::SampledSpectrum<T, N>, radiometry::SampledSpectrum<T, N>, SensorResponseSpectrumType>(
            radiance,
            sampled_illuminant,
            wavelengths,
            pdf,
            m_sensor_response
        );
        return sensor_rgb * m_image_ratio;
    }

    /**
     * @brief Get the 3×3 matrix mapping sensor RGB to CIE XYZ.
     *
     * The matrix is either obtained by least-squares fitting on reference
     * color swatches or constructed as a white-balance/identity matrix,
     * depending on which constructor was used.
     *
     * @return Sensor-RGB-to-XYZ transformation matrix.
     */
    math::Matrix<T, 3, 3> sensor_rgb_to_xyz_matrix() const {
        return m_sensor_rgb_to_xyz;
    }

    /**
     * @brief Transform sensor RGB values to CIE XYZ using a 3×3 matrix.
     *
     * Applies the precomputed 3×3 matrix so that the output XYZ is
     * a linear transform of the input sensor RGB.
     *
     * @param sensor_rgb Sensor RGB values.
     * @return Corresponding CIE XYZ tristimulus values.
     */
    radiometry::XYZ<T> sensor_rgb_to_xyz(const radiometry::RGB<T>& sensor_rgb) const {
        return m_sensor_rgb_to_xyz * sensor_rgb;
    }

    /**
     * @brief Transform sensor RGB values directly to the target RGB color space.
     *
     * This is equivalent to first converting to XYZ using
     * sensor_rgb_to_xyz() and then applying the RGB color space's
     * inverse transformation to obtain linear RGB values.
     *
     * @param sensor_rgb Sensor RGB values.
     * @return Linear RGB values in the target color space.
     */
    radiometry::RGB<T> sensor_rgb_to_color_space_rgb(const radiometry::RGB<T>& sensor_rgb) const {
        radiometry::XYZ<T> xyz = sensor_rgb_to_xyz(sensor_rgb);
        return m_color_space.to_rgb(xyz);
    }

};

};

```

## projective_camera.hpp

```hpp
/**
 * @file
 * @brief Projective camera models (orthographic, perspective, thin-lens).
 */
#pragma once

#include <cmath>

#include "geometry/ray.hpp"
#include "geometry/transform.hpp"

#include "sampler/2d.hpp"
#include "math/point.hpp"
#include "math/vector.hpp"

#include "camera.hpp"

namespace pbpt::camera {

/**
 * @brief Camera-space projection utilities.
 *
 * Encapsulates the standard graphics pipeline transforms:
 * camera space -> clip space -> viewport (raster) space. It also provides
 * helper factories for orthographic and perspective projections.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
class CameraProjection {
private:
    /// Transform from clip space to viewport (raster) coordinates.
    geometry::Transform<T> m_clip_to_viewport{};
    /// Transform from camera space to clip space.
    geometry::Transform<T> m_camera_to_clip{};
    /// Precomputed transform from camera space directly to viewport space.
    geometry::Transform<T> m_camera_to_viewport{};

public:
    /**
     * @brief Construct a projection from camera-to-clip and clip-to-viewport.
     *
     * The camera-to-viewport transform is computed as
     * `clip_to_viewport * camera_to_clip`.
     *
     * @param camera_to_clip   Transform from camera space to clip space.
     * @param clip_to_viewport Transform from clip space to viewport space.
     */
    CameraProjection(
        const geometry::Transform<T>& camera_to_clip, 
        const geometry::Transform<T>& clip_to_viewport
    ) : m_clip_to_viewport(clip_to_viewport), m_camera_to_clip(camera_to_clip) {
        m_camera_to_viewport = m_clip_to_viewport * m_camera_to_clip;
    }

    /// Get the camera-to-clip transform.
    geometry::Transform<T> camera_to_clip() const {
        return m_camera_to_clip;
    }

    /// Get the clip-to-viewport transform.
    geometry::Transform<T> clip_to_viewport() const {
        return m_clip_to_viewport;
    }

    /// Get the camera-to-viewport transform.
    geometry::Transform<T> camera_to_viewport() const {
        return m_camera_to_viewport;
    }

    /// Get the inverse transform: viewport-to-clip.
    geometry::Transform<T> viewport_to_clip() const {
        return m_clip_to_viewport.inversed();
    }

    /// Get the inverse transform: clip-to-camera.
    geometry::Transform<T> clip_to_camera() const {
        return m_camera_to_clip.inversed();
    }

    /// Get the inverse transform: viewport-to-camera.
    geometry::Transform<T> viewport_to_camera() const {
        return m_camera_to_viewport.inversed();
    }

    /**
     * @brief Map a 2D viewport coordinate into camera space at z = 0.
     *
     * @param p 2D viewport coordinate.
     * @return 3D point in camera space on the z = 0 plane.
     */
    math::Point<T, 3> apply_viewport_to_camera(const math::Point<T, 2>& p, T viewport_depth = T{0.0}) const {
        math::Homogeneous<T, 4> hp = math::Homogeneous<T, 4>::from_point(math::Point<T, 3>(p.x(), p.y(), viewport_depth));
        auto hc = viewport_to_camera().transform_homogeneous(hp);
        return hc.to_point();
    }

    /**
     * @brief Build an orthographic projection.
     *
     * The view volume is defined by left/right, bottom/top and near/far
     * planes in camera space, and the viewport transform maps the clip
     * cube to a raster of given width and height.
     *
     * @param left   Left plane in camera x.
     * @param right  Right plane in camera x.
     * @param bottom Bottom plane in camera y.
     * @param top    Top plane in camera y.
     * @param near   Near plane in camera z.
     * @param far    Far plane in camera z.
     * @param width  Viewport width in pixels.
     * @param height Viewport height in pixels.
     * @return CameraProjection representing this orthographic setup.
     */
    static CameraProjection<T> orthographic(
        T left, T right, T bottom, T top, T near, T far, 
        T width, T height
    ) {
        geometry::Transform<T> camera_to_clip = geometry::Transform<T>::orthographic(left, right, bottom, top, near, far);
        geometry::Transform<T> clip_to_viewport = geometry::Transform<T>::viewport(width, height);
        return CameraProjection<T>(camera_to_clip, clip_to_viewport);
    }

    /**
     * @brief Build a perspective projection.
     *
     * This uses a vertical field of view `fov_y_rad` (in radians),
     * aspect ratio, and near/far clipping planes, followed by a viewport
     * transform to raster coordinates.
     *
     * @param fov_y_rad Vertical field of view in radians.
     * @param aspect_xy Aspect ratio (width / height).
     * @param near      Near plane in camera z.
     * @param far       Far plane in camera z.
     * @param width     Viewport width in pixels.
     * @param height    Viewport height in pixels.
     * @return CameraProjection representing this perspective setup.
     */
    static CameraProjection<T> perspective(
        T fov_y_rad, T aspect_xy, T near, T far, 
        T width, T height
    ) {
        geometry::Transform<T> camera_to_clip = geometry::Transform<T>::perspective(fov_y_rad, aspect_xy, near, far);
        geometry::Transform<T> clip_to_viewport = geometry::Transform<T>::viewport(width, height);
        return CameraProjection<T>(camera_to_clip, clip_to_viewport);
    }
};

/**
 * @brief Base class for projective cameras (orthographic, perspective).
 *
 * This class combines the generic `Camera` interface with a
 * `CameraProjection` object that defines how camera space maps to
 * clip space and viewport space.
 *
 * @tparam Derived Concrete camera type.
 * @tparam T       Scalar type (e.g. float or double).
 */
template<typename Derived, typename T>
class ProjectiveCamera : public Camera<Derived, T> {
    friend class Camera<Derived, T>;
protected:
    /// Projection used by this camera.
    CameraProjection<T> m_projection{};

public:
    /**
     * @brief Construct a projective camera with the given projection.
     *
     * @param projection CameraProjection defining the projection.
     */
    ProjectiveCamera(const CameraProjection<T>& projection) : m_projection(projection) {}

    /// Get the camera projection.
    const CameraProjection<T>& projection() const {
        return m_projection;
    }

protected:
    /**
     * @brief Implementation of film resolution query.
     *
     * The resolution is derived from the viewport transform, assuming
     * that the viewport matrix scales the clip space cube to pixel
     * coordinates. The width and height are inferred from the diagonal
     * elements of the viewport transform.
     *
     * @return 2D vector (width, height) in pixels.
     */
    math::Vector<int, 2> film_resolution_impl() const {
        const auto viewport = m_projection.clip_to_viewport();
        const auto& mat = viewport.matrix();
        auto width_value = mat.at(0, 0) * T(2);
        auto height_value = mat.at(1, 1) * T(2);
        const int width = std::max(1, static_cast<int>(std::lround(static_cast<double>(width_value))));
        const int height = std::max(1, static_cast<int>(std::lround(static_cast<double>(height_value))));
        return math::Vector<int, 2>(width, height);
    }
};

/**
 * @brief Helper for creating an orthographic projection from film parameters.
 *
 * The physical film size defines the extents in x and y; near/far specify
 * depth range, and `film_resolution` gives the raster size.
 */
template<typename T>
inline CameraProjection<T> create_orthographic_projection(
    const math::Vector<int, 2>& film_resolution,
    const math::Vector<T, 2>& film_physical_size,
    T near, T far
) {
    return CameraProjection<T>::orthographic(
        -film_physical_size.x() / 2,
        film_physical_size.x() / 2,
        -film_physical_size.y() / 2,
        film_physical_size.y() / 2,
        near,
        far,
        film_resolution.x(),
        film_resolution.y()
    );
}

/**
 * @brief Helper for creating a perspective projection from film parameters.
 *
 * The vertical field of view is computed from the film height and
 * near plane distance so that the film edges align with the frustum
 * at the near plane.
 */
template<typename T>
inline CameraProjection<T> create_perspective_projection(
    const math::Vector<int, 2>& film_resolution,
    const math::Vector<T, 2>& film_physical_size,
    T near, T far
) {
    T aspect = film_physical_size.x() / film_physical_size.y();
    T fov_y = 2 * std::atan2(film_physical_size.y() / 2, near);
    return CameraProjection<T>::perspective(
        fov_y,
        aspect,
        near,
        far,
        film_resolution.x(),
        film_resolution.y()
    );
}

/**
 * @brief Ideal orthographic (pinhole) camera.
 *
 * Rays are cast with a constant direction along +z and origins located
 * at the back-projected film positions. There is no perspective foreshortening.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
class OrthographicCamera : public ProjectiveCamera<OrthographicCamera<T>, T> {
    friend class Camera<OrthographicCamera<T>, T>;
    friend class ProjectiveCamera<OrthographicCamera<T>, T>;

public:
    /**
     * @brief Construct an orthographic camera from explicit bounds and resolution.
     *
     * @param left         Left plane in camera x.
     * @param right        Right plane in camera x.
     * @param bottom       Bottom plane in camera y.
     * @param top          Top plane in camera y.
     * @param near         Near plane in camera z.
     * @param far          Far plane in camera z.
     * @param resolution_x Film resolution in x (pixels).
     * @param resolution_y Film resolution in y (pixels).
     */
    OrthographicCamera(
        T left, T right, T bottom, T top, T near, T far, 
        T resolution_x, T resolution_y
    ) : ProjectiveCamera<OrthographicCamera<T>, T>(CameraProjection<T>::orthographic(
        left, right, bottom, top, near, far, 
        resolution_x, resolution_y)
    ) { }

    /**
     * @brief Construct an orthographic camera from film parameters.
     *
     * @param film_resolution   Film resolution (width, height) in pixels.
     * @param film_physical_size Physical film size in x and y.
     * @param near              Near plane in camera z.
     * @param far               Far plane in camera z.
     */
    OrthographicCamera(
        const math::Vector<int, 2>& film_resolution, 
        const math::Vector<T, 2>& film_physical_size,
        T near, T far
    ) : ProjectiveCamera<OrthographicCamera<T>, T>(
        create_orthographic_projection(
            film_resolution, 
            film_physical_size, 
            near, far
        )
    ) {}

private:
    /// Generate a primary ray for an orthographic camera.
    geometry::Ray<T, 3> generate_ray_impl(const CameraSample<T>& sample) const {
        auto origin = this->m_projection.apply_viewport_to_camera(sample.p_film);
        auto direction = math::Vector<T, 3>(0, 0, -1);
        return geometry::Ray<T, 3>(origin, direction);
    }

    /// Generate ray differentials by perturbing the film sample position.
    geometry::RayDifferential<T, 3> generate_differential_ray_impl(const CameraSample<T>& sample) const {
        geometry::Ray<T, 3> main_ray = this->generate_ray_impl(sample);

        T eps = static_cast<T>(1e-3);
        auto p_film_x = sample.p_film + math::Vector<T, 2>(eps, 0); 
        auto p_film_y = sample.p_film + math::Vector<T, 2>(0, eps);

        CameraSample<T> sample_x = { p_film_x, sample.p_lens };
        CameraSample<T> sample_y = { p_film_y, sample.p_lens };

        geometry::Ray<T, 3> ray_x = this->generate_ray_impl(sample_x);
        geometry::Ray<T, 3> ray_y = this->generate_ray_impl(sample_y);

        return geometry::RayDifferential<T, 3>(main_ray, {ray_x, ray_y});
    }
};

/**
 * @brief Ideal perspective (pinhole) camera.
 *
 * Rays originate at the camera origin and pass through film points
 * back-projected into camera space using the perspective projection.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
class PerspectiveCamera : public ProjectiveCamera<PerspectiveCamera<T>, T> {
    friend class Camera<PerspectiveCamera<T>, T>;
    friend class ProjectiveCamera<PerspectiveCamera<T>, T>;

public:
    /**
     * @brief Construct a perspective camera from explicit frustum and resolution.
     *
     * @param fov_y_rad    Vertical field of view in radians.
     * @param aspect_xy    Aspect ratio (width / height).
     * @param near         Near plane in camera z.
     * @param far          Far plane in camera z.
     * @param resolution_x Film resolution in x (pixels).
     * @param resolution_y Film resolution in y (pixels).
     */
    PerspectiveCamera(
        const T fov_y_rad,
        const T aspect_xy,
        const T near, const T far,
        const T resolution_x, const T resolution_y
    ) : ProjectiveCamera<PerspectiveCamera<T>, T>(CameraProjection<T>::perspective(
        fov_y_rad, 
        aspect_xy, 
        near, far, 
        resolution_x, resolution_y)
    ) { }

    /**
     * @brief Construct a perspective camera from film parameters.
     *
     * The field of view is derived from the film height and near plane
     * distance so that the film edges align with the frustum at the
     * near plane.
     *
     * @param film_resolution   Film resolution (width, height) in pixels.
     * @param film_physical_size Physical film size in x and y.
     * @param near              Near plane in camera z.
     * @param far               Far plane in camera z.
     */
    PerspectiveCamera(
        const math::Vector<int, 2>& film_resolution, 
        const math::Vector<T, 2>& film_physical_size,
        T near, T far
    ) : ProjectiveCamera<PerspectiveCamera<T>, T>(
        create_perspective_projection(film_resolution, film_physical_size, near, far)
    ) {}

private:
    /// Generate a primary ray for a perspective camera.
    geometry::Ray<T, 3> generate_ray_impl(const CameraSample<T>& sample) const {
        auto origin = math::Point<T, 3>(0, 0, 0);
        auto p_camera = this->m_projection.apply_viewport_to_camera(sample.p_film);
        return geometry::Ray<T, 3>(origin, p_camera);
    }

    /// Generate ray differentials by perturbing the film sample position.
    geometry::RayDifferential<T, 3> generate_differential_ray_impl(const CameraSample<T>& sample) const {
        
        geometry::Ray<T, 3> main_ray = this->generate_ray_impl(sample);

        T eps = static_cast<T>(1e-3);
        auto p_film_x = sample.p_film + math::Vector<T, 2>(eps, 0); 
        auto p_film_y = sample.p_film + math::Vector<T, 2>(0, eps);

        CameraSample<T> sample_x = { p_film_x, sample.p_lens };
        CameraSample<T> sample_y = { p_film_y, sample.p_lens };

        geometry::Ray<T, 3> ray_x = this->generate_ray_impl(sample_x);
        geometry::Ray<T, 3> ray_y = this->generate_ray_impl(sample_y);

        return geometry::RayDifferential<T, 3>(main_ray, {ray_x, ray_y});
    }
};

// Thin Lens Cameras

/**
 * @brief Orthographic camera with a finite circular aperture (thin lens).
 *
 * Instead of originating rays from a fixed point behind the film, this
 * camera simulates depth of field by sampling points on a circular lens
 * and focusing rays on a plane at a given focal distance.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
class ThinLensOrthographicCamera : public ProjectiveCamera<ThinLensOrthographicCamera<T>, T> {
    friend class Camera<ThinLensOrthographicCamera<T>, T>;
    friend class ProjectiveCamera<ThinLensOrthographicCamera<T>, T>;

private:
    /// Lens radius controlling aperture size (larger radius -> shallower depth of field).
    T m_lens_radius{0};
    /// Distance from the lens to the focal plane along +z.
    T m_focal_distance{1};

public:

    /**
     * @brief Construct a thin-lens orthographic camera from explicit bounds.
     *
     * @param left          Left plane in camera x.
     * @param right         Right plane in camera x.
     * @param bottom        Bottom plane in camera y.
     * @param top           Top plane in camera y.
     * @param near          Near plane in camera z.
     * @param far           Far plane in camera z.
     * @param resolution_x  Film resolution in x (pixels).
     * @param resolution_y  Film resolution in y (pixels).
     * @param lens_radius   Radius of the circular lens aperture.
     * @param focal_distance Distance from lens to focal plane along +z.
     */
    ThinLensOrthographicCamera(
        T left, T right, T bottom, T top, T near, T far, 
        T resolution_x, T resolution_y,
        T lens_radius, T focal_distance
    ) : ProjectiveCamera<ThinLensOrthographicCamera<T>, T>(CameraProjection<T>::orthographic(
        left, right, bottom, top, near, far, 
        resolution_x, resolution_y)
    ), m_lens_radius(lens_radius), m_focal_distance(focal_distance) { }

    /**
     * @brief Construct a thin-lens orthographic camera from film parameters.
     *
     * @param film_resolution    Film resolution (width, height) in pixels.
     * @param film_physical_size Physical film size in x and y.
     * @param near               Near plane in camera z.
     * @param far                Far plane in camera z.
     * @param lens_radius        Radius of the circular lens aperture.
     * @param focal_distance     Distance from lens to focal plane along +z.
     */
    ThinLensOrthographicCamera(
        const math::Vector<int, 2>& film_resolution, 
        const math::Vector<T, 2>& film_physical_size,
        T near, T far, T lens_radius, T focal_distance
    ) : ProjectiveCamera<ThinLensOrthographicCamera<T>, T>(
            create_orthographic_projection(
                film_resolution, 
                film_physical_size, 
                near, far
            )
        ), m_lens_radius(lens_radius), 
            m_focal_distance(focal_distance) {}
    
private:
    /**
     * @brief Generate a ray using a thin-lens orthographic model.
     *
     * The algorithm:
     * 1. Cast a pinhole ray from the film point with direction +z.
     * 2. Intersect this ray with a plane at z = focal_distance to find
     *    the focus point.
     * 3. Sample a point on the lens disk and shoot a ray from the lens
     *    point to the focus point.
     */
    geometry::Ray<T, 3> generate_ray_impl(const CameraSample<T>& sample) const {
        // 1. 获取视口映射点 (位于近平面，但这不重要，我们只需要它的 x 和 y)
        auto p_camera = this->m_projection.apply_viewport_to_camera(sample.p_film);

        // 2. [优化] 直接构造焦点
        // 正交投影特性：焦点产生的 X,Y 与胶片点一致。
        // 焦平面深度：严格位于 Z = -m_focal_distance
        auto p_focus = math::Point<T, 3>(p_camera.x(), p_camera.y(), -m_focal_distance);

        // 3. 采样透镜 (Lens)
        auto p_lens = sampler::sample_uniform_disk_concentric(sample.p_lens, m_lens_radius);
        auto origin = math::Point<T, 3>(p_lens.x(), p_lens.y(), 0);

        // 4. 生成射线
        auto direction = (p_focus - origin).normalized();
        return geometry::Ray<T, 3>(origin, direction);
    }

    /// Generate ray differentials by perturbing the film sample position.
    geometry::RayDifferential<T, 3> generate_differential_ray_impl(const CameraSample<T>& sample) const {
       
        geometry::Ray<T, 3> main_ray = this->generate_ray_impl(sample);

        T eps = static_cast<T>(1e-3);
        auto p_film_x = sample.p_film + math::Vector<T, 2>(eps, 0); 
        auto p_film_y = sample.p_film + math::Vector<T, 2>(0, eps);

        CameraSample<T> sample_x = { p_film_x, sample.p_lens };
        CameraSample<T> sample_y = { p_film_y, sample.p_lens };

        geometry::Ray<T, 3> ray_x = this->generate_ray_impl(sample_x);
        geometry::Ray<T, 3> ray_y = this->generate_ray_impl(sample_y);

        return geometry::RayDifferential<T, 3>(main_ray, {ray_x, ray_y});
    }
};

/**
 * @brief Perspective camera with a finite circular aperture (thin lens).
 *
 * This model adds depth of field to a pinhole perspective camera by
 * sampling points on a circular lens aperture and focusing on a plane
 * at `focal_distance`.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
class ThinLensPerspectiveCamera : public ProjectiveCamera<ThinLensPerspectiveCamera<T>, T>{
    friend class Camera<ThinLensPerspectiveCamera<T>, T>;
    friend class ProjectiveCamera<ThinLensPerspectiveCamera<T>, T>;

private:
    /// Lens radius controlling aperture size (larger radius -> shallower depth of field).
    T m_lens_radius{0};
    /// Distance from the lens to the focal plane along +z.
    T m_focal_distance{1};
    
public:
    /**
     * @brief Construct a thin-lens perspective camera from explicit frustum.
     *
     * @param fov_y_rad     Vertical field of view in radians.
     * @param aspect_xy     Aspect ratio (width / height).
     * @param near          Near plane in camera z.
     * @param far           Far plane in camera z.
     * @param resolution_x  Film resolution in x (pixels).
     * @param resolution_y  Film resolution in y (pixels).
     * @param lens_radius   Radius of the circular lens aperture.
     * @param focal_distance Distance from lens to focal plane along +z.
     */
    ThinLensPerspectiveCamera(
        const T fov_y_rad,
        const T aspect_xy,
        const T near, const T far,
        const T resolution_x, const T resolution_y,
        T lens_radius, T focal_distance
    ) : ProjectiveCamera<ThinLensPerspectiveCamera<T>, T>(CameraProjection<T>::perspective(
        fov_y_rad, 
        aspect_xy, 
        near, far, 
        resolution_x, resolution_y)
    ), m_lens_radius(lens_radius), m_focal_distance(focal_distance) { }


    /**
     * @brief Construct a thin-lens perspective camera from film parameters.
     *
     * The perspective projection is derived from the film size and near
     * plane; depth of field is controlled by lens_radius and focal_distance.
     *
     * @param film_resolution    Film resolution (width, height) in pixels.
     * @param film_physical_size Physical film size in x and y.
     * @param near               Near plane in camera z.
     * @param far                Far plane in camera z.
     * @param lens_radius        Radius of the circular lens aperture.
     * @param focal_distance     Distance from lens to focal plane along +z.
     */
    ThinLensPerspectiveCamera(
        const math::Vector<int, 2>& film_resolution, 
        const math::Vector<T, 2>& film_physical_size,
        T near, T far, T lens_radius, T focal_distance
    ) : ProjectiveCamera<ThinLensPerspectiveCamera<T>, T>(
            create_perspective_projection(film_resolution, film_physical_size, near, far)
        ), 
        m_lens_radius(lens_radius), 
        m_focal_distance(focal_distance) {}

private:
    /**
     * @brief Generate a ray using a thin-lens perspective model.
     *
     * The algorithm:
     * 1. Compute the pinhole direction from the origin through the
     *    back-projected film point in camera space.
     * 2. Intersect this ray with a plane at z = focal_distance to
     *    obtain the focus point.
     * 3. Sample a point on a disk of radius m_lens_radius in the lens
     *    plane (z = 0) and shoot a ray from that point to the focus point.
     */
    geometry::Ray<T, 3> generate_ray_impl(const CameraSample<T>& sample) const {
        auto p_camera = this->m_projection.apply_viewport_to_camera(sample.p_film);
        auto p_lens = sampler::sample_uniform_disk_concentric(sample.p_lens, m_lens_radius);
        auto origin = math::Point<T, 3>(p_lens.x(), p_lens.y(), 0);
        auto pinhole_ray = geometry::Ray<T, 3>(math::Point<T, 3>(0, 0, 0), p_camera);
        T t = -m_focal_distance / pinhole_ray.direction().z();
        auto p_focus = pinhole_ray.at(t);
        auto direction = (p_focus - origin).normalized();
        return geometry::Ray<T, 3>(origin, direction);
    }

    /// Generate ray differentials by perturbing the film sample position.
    geometry::RayDifferential<T, 3> generate_differential_ray_impl(const CameraSample<T>& sample) const {
  
        geometry::Ray<T, 3> main_ray = this->generate_ray_impl(sample);

        T eps = static_cast<T>(1e-3);
        auto p_film_x = sample.p_film + math::Vector<T, 2>(eps, 0); 
        auto p_film_y = sample.p_film + math::Vector<T, 2>(0, eps);

        CameraSample<T> sample_x = { p_film_x, sample.p_lens };
        CameraSample<T> sample_y = { p_film_y, sample.p_lens };

        geometry::Ray<T, 3> ray_x = this->generate_ray_impl(sample_x);
        geometry::Ray<T, 3> ray_y = this->generate_ray_impl(sample_y);

        return geometry::RayDifferential<T, 3>(main_ray, {ray_x, ray_y});
    }
};

} // namespace pbpt::camera

```

## render_transform.hpp

```hpp
/**
 * @file
 * @brief Helpers for choosing the coordinate space used for rendering.
 */
#pragma once

#include "geometry/transform.hpp"

namespace pbpt::camera {

/**
 * @brief Coordinate space where rendering computations are performed.
 *
 * - Camera: rendering is done in camera space; camera_to_render is identity.
 * - CameraWorld: rendering space separates local camera rotation/scale
 *   from world translation (useful for improving numerical stability).
 * - World: rendering is done directly in world space.
 */
enum class RenderSpace{
    Camera, 
    CameraWorld,
    World
};

/**
 * @brief Manages transforms between camera, render, and world spaces.
 *
 * Given a camera-to-world transform and a chosen RenderSpace, this class
 * decomposes or combines transforms so that all three spaces are easily
 * accessible. This is useful when you want to do shading in a space that
 * avoids large world-space translations or when you want to keep camera
 * space and world space separate.
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
class RenderTransform {  
private:
    /// Transform from camera space to render space.
    geometry::Transform<T> m_camera_to_render{};
    /// Transform from render space to world space.
    geometry::Transform<T> m_render_to_world{};
    /// Original camera-to-world transform.
    geometry::Transform<T> m_camera_to_world{};
    /// Currently selected render space.
    RenderSpace m_render_space{RenderSpace::CameraWorld};

public:
    /// Create from a camera-to-world transform.
    static RenderTransform<T> from_camera_to_world(
        const geometry::Transform<T>& camera_to_world, 
        RenderSpace space = RenderSpace::CameraWorld
    ) {
        return RenderTransform<T>(camera_to_world, space);
    }

    /// Create from a world-to-camera transform.
    static RenderTransform<T> from_world_to_camera(
        const geometry::Transform<T>& world_to_camera, 
        RenderSpace space = RenderSpace::CameraWorld
    ) {
        return RenderTransform<T>(world_to_camera.inversed(), space);
    }

public:
    /**
     * @brief Construct render transforms from a camera-to-world transform.
     *
     * Depending on @p space:
     * - Camera: render space equals camera space.
     * - CameraWorld: render space contains rotation and scale, while
     *   world space contains the translation component.
     * - World: render space equals world space.
     */
    RenderTransform(
        const geometry::Transform<T>& camera_to_world, 
        RenderSpace space = RenderSpace::CameraWorld
    ) : m_camera_to_world(camera_to_world), m_render_space(space) {
        if (space == RenderSpace::Camera) {
            m_camera_to_render = geometry::Transform<T>::identity();
            m_render_to_world = camera_to_world;
        } else if (space == RenderSpace::CameraWorld) {
            auto [translation, rotation, scale] = fast_decompose_transform(camera_to_world);
            m_camera_to_render = rotation * scale;
            m_render_to_world = translation;
        } else if (space == RenderSpace::World) {
            m_camera_to_render = camera_to_world;
            m_render_to_world = geometry::Transform<T>::identity();
        }
    }

    /// Get the current render space mode.
    RenderSpace render_space() const {
        return m_render_space;
    }

    /// Transform from camera space to render space.
    geometry::Transform<T> camera_to_render() const {
        return m_camera_to_render;
    }

    /// Transform from render space back to camera space.
    geometry::Transform<T> render_to_camera() const {
        return m_camera_to_render.inversed();
    }

    /// Transform from render space to world space.
    geometry::Transform<T> render_to_world() const {
        return m_render_to_world;
    }

    /// Transform from world space to render space.
    geometry::Transform<T> world_to_render() const {
        return m_render_to_world.inversed();
    }

    /// Original transform from camera space to world space.
    geometry::Transform<T> camera_to_world() const {
        return m_camera_to_world;
    }

    /// Transform from world space back to camera space.
    geometry::Transform<T> world_to_camera() const {
        return m_camera_to_world.inversed();
    }

    /**
     * @brief Convert an object-to-world transform into object-to-render.
     *
     * Useful for building shapes in world space while shading in the chosen
     * render space.
     */
    geometry::Transform<T> object_to_render_from_object_to_world(
        const geometry::Transform<T>& object_to_world
    ) const {
        return world_to_render() * object_to_world;
    }

    /**
     * @brief Change the render space in-place.
     *
     * This recomputes internal transforms while preserving the original
     * camera-to-world transform.
     */
    RenderTransform<T>& change_render_space(RenderSpace space) {
        *this = to_render_space(space);
        return *this;
    }

    /**
     * @brief Create a new RenderTransform in a different render space.
     *
     * @param space Target render space.
     * @return New RenderTransform using the same camera-to-world transform.
     */
    RenderTransform<T> to_render_space(RenderSpace space) const {
        return RenderTransform<T>(m_camera_to_world, space);
    }
};

}; // namespace pbpt::camera

```

## spherical_camera.hpp

```hpp
/**
 * @file
 * @brief Spherical (environment) camera that maps directions to a 2D image.
 */
#pragma once

#include <utility>

#include "camera.hpp"
#include "geometry/ray.hpp"
#include "geometry/spherical.hpp"
#include "math/function.hpp"
#include "math/point.hpp"
#include "math/vector.hpp"

namespace pbpt::camera {

/**
 * @brief Mapping used by the spherical camera.
 *
 * - EqualRectangular: image coordinates map linearly to spherical angles
 *   (theta, phi), like a standard latitude-longitude environment map.
 * - EqualArea: image coordinates are warped so that each pixel covers
 *   approximately equal area on the unit sphere.
 */
enum class SphericalCameraMapping {
    EqualRectangular,
    EqualArea
};

/**
 * @brief Spherical camera that generates rays covering the full sphere.
 *
 * The spherical camera maps film coordinates to directions on the unit
 * sphere. Depending on the selected mapping, the sampling may be uniform
 * in angle (equirectangular) or approximately uniform in solid angle
 * (equal-area).
 *
 * @tparam T Scalar type (e.g. float or double).
 */
template<typename T>
class SphericalCamera : public Camera<SphericalCamera<T>, T> {
    friend class Camera<SphericalCamera<T>, T>;

private:
    /// Mapping from film coordinates to directions.
    SphericalCameraMapping m_mapping{SphericalCameraMapping::EqualRectangular};
    /// Film resolution in pixels (width, height).
    math::Vector<int, 2> m_film_resolution{1920, 960};

public:
    /**
     * @brief Construct a spherical camera.
     *
     * @param film_resolution Film resolution (width, height).
     * @param mapping         Mapping from image plane to sphere.
     */
    SphericalCamera(
        const math::Vector<T, 2>& film_resolution,
        SphericalCameraMapping mapping = SphericalCameraMapping::EqualRectangular)
        : m_mapping(mapping), m_film_resolution(film_resolution) {}

    /// Get the current spherical mapping mode.
    SphericalCameraMapping mapping() const {
        return m_mapping;
    }

private:
    /**
     * @brief Generate a ray for the spherical camera.
     *
     * The algorithm:
     * 1. Normalize film coordinates to [0, 1] to obtain uv.
     * 2. For EqualRectangular: convert uv to spherical angles
     *    theta = pi * u, phi = 2 * pi * v, then to a direction vector.
     * 3. For EqualArea: warp uv to an equal-area square and then map to
     *    a direction on the unit sphere.
     * 4. Swap y and z components so that the camera's up and forward
     *    directions match the chosen convention.
     */
    geometry::Ray<T, 3> generate_ray_impl(const CameraSample<T>& sample) const {
        math::Point<T, 2> uv = math::Point<T, 2>(
            sample.p_film.x() / static_cast<T>(m_film_resolution.x()),
            sample.p_film.y() / static_cast<T>(m_film_resolution.y())
        );
        T theta = math::pi_v<T> * uv.x(), phi = 2 * math::pi_v<T> * uv.y();

        math::Vector<T, 3> dir;
        if (m_mapping == SphericalCameraMapping::EqualRectangular) {
            dir = geometry::SphericalPoint<T, 3>(math::Vector<T, 2>{theta, phi}, 1).to_cartesian();
        } else if (m_mapping == SphericalCameraMapping::EqualArea) {
            dir = geometry::equal_area_square_to_sphere(
                geometry::warp_equal_area_square(uv).to_vector()
            );
        }
        std::swap(dir.y(), dir.z());
        return geometry::Ray<T, 3>(math::Point<T, 3>(0, 0, 0), dir);
    }

    /// Generate ray differentials by perturbing the film sample position.
    geometry::RayDifferential<T, 3> generate_differential_ray_impl(const CameraSample<T>& sample) const {
        geometry::Ray<T, 3> main_ray = this->generate_ray_impl(sample);

        T eps = static_cast<T>(1e-3);
        auto p_film_x = sample.p_film + math::Vector<T, 2>(eps, 0); 
        auto p_film_y = sample.p_film + math::Vector<T, 2>(0, eps);

        CameraSample<T> sample_x = { p_film_x, sample.p_lens };
        CameraSample<T> sample_y = { p_film_y, sample.p_lens };

        geometry::Ray<T, 3> ray_x = this->generate_ray_impl(sample_x);
        geometry::Ray<T, 3> ray_y = this->generate_ray_impl(sample_y);

        return geometry::RayDifferential<T, 3>(main_ray, {ray_x, ray_y});
    }

    /// Return the film resolution in pixels.
    math::Vector<int, 2> film_resolution_impl() const {
        return m_film_resolution;
    }
};


}; // namespace pbpt::camera

```